{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import flet as ft\n",
    "import os\n",
    "\n",
    "import base64\n",
    "from pylsl import StreamInlet, resolve_stream\n",
    "from scipy.signal import welch, spectrogram\n",
    "import pyxdf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxdf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def concat_data(frequency):\n",
    "    # โฟลเดอร์ที่เก็บข้อมูล\n",
    "    data_folder = f'../../../data_ssvep/Toey/SSVEP_data/{frequency}/'\n",
    "\n",
    "    # เก็บข้อมูลจากทุกไฟล์ในโฟลเดอร์\n",
    "    all_data = []\n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        streams, _ = pyxdf.load_xdf(file_path)\n",
    "        raw_data = streams[0][\"time_series\"].T\n",
    "        all_data.append(raw_data)\n",
    "\n",
    "    # แปลงเป็น NumPy array และรวมข้อมูลด้วย np.concatenate\n",
    "    all_data_array = np.concatenate(all_data, axis=1)\n",
    "\n",
    "    return all_data_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 144520)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m raw_data1 \u001b[38;5;241m=\u001b[39m concat_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m6Hz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_data1\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m raw_data2 \u001b[38;5;241m=\u001b[39m concat_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20Hz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(raw_data2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m raw_data3 \u001b[38;5;241m=\u001b[39m concat_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0Hz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[58], line 13\u001b[0m, in \u001b[0;36mconcat_data\u001b[1;34m(frequency)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(data_folder):\n\u001b[0;32m     12\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, file_name)\n\u001b[1;32m---> 13\u001b[0m     streams, _ \u001b[38;5;241m=\u001b[39m pyxdf\u001b[38;5;241m.\u001b[39mload_xdf(file_path)\n\u001b[0;32m     14\u001b[0m     raw_data \u001b[38;5;241m=\u001b[39m streams[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_series\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     15\u001b[0m     all_data\u001b[38;5;241m.\u001b[39mappend(raw_data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyxdf\\pyxdf.py:308\u001b[0m, in \u001b[0;36mload_xdf\u001b[1;34m(filename, select_streams, on_chunk, synchronize_clocks, handle_clock_resets, dejitter_timestamps, jitter_break_threshold_seconds, jitter_break_threshold_samples, clock_reset_threshold_seconds, clock_reset_threshold_stds, clock_reset_threshold_offset_seconds, clock_reset_threshold_offset_stds, winsor_threshold, verbose)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tag \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;66;03m# read [Samples] chunk...\u001b[39;00m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;66;03m# noinspection PyBroadException\u001b[39;00m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m         nsamples, stamps, values \u001b[38;5;241m=\u001b[39m _read_chunk3(f, temp[StreamId])\n\u001b[0;32m    309\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    310\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  reading [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (temp[StreamId]\u001b[38;5;241m.\u001b[39mnchns, nsamples)\n\u001b[0;32m    311\u001b[0m         )\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# optionally send through the on_chunk function\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyxdf\\pyxdf.py:471\u001b[0m, in \u001b[0;36m_read_chunk3\u001b[1;34m(f, s)\u001b[0m\n\u001b[0;32m    468\u001b[0m         f\u001b[38;5;241m.\u001b[39mreadinto(raw)\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;66;03m# no fromfile(), see\u001b[39;00m\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;66;03m# https://github.com/numpy/numpy/issues/13319\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m         values[k, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(raw, dtype\u001b[38;5;241m=\u001b[39ms\u001b[38;5;241m.\u001b[39mdtype, count\u001b[38;5;241m=\u001b[39ms\u001b[38;5;241m.\u001b[39mnchns)\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nsamples, stamps, values\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raw_data1 = concat_data('6Hz')\n",
    "print(raw_data1.shape)\n",
    "raw_data2 = concat_data('20Hz')\n",
    "print(raw_data2.shape)\n",
    "raw_data3 = concat_data('0Hz')\n",
    "print(raw_data3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streams1, header = pyxdf.load_xdf('../../../data_ssvep/Toey/SSVEP_data/6Hz/6hz_1')\n",
    "# raw_data1 = streams1[0][\"time_series\"].T #From Steam variable this query is EEG data\n",
    "# print(raw_data1.shape)\n",
    "\n",
    "# streams2, header = pyxdf.load_xdf('../../../data_ssvep/Toey/SSVEP_data/20Hz/20hz_1')\n",
    "# raw_data2 = streams2[0][\"time_series\"].T #From Steam variable this query is EEG data\n",
    "\n",
    "# streams3, header = pyxdf.load_xdf('../../../data_ssvep/Toey/SSVEP_data/0Hz/0hz_1')\n",
    "# raw_data3 = streams3[0][\"time_series\"].T #From Steam variable this query is EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = raw_data1[0:4,:]\n",
    "data2 = raw_data2[0:4,:]\n",
    "data3 = raw_data3[0:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144520\n",
      "164360\n",
      "137610\n"
     ]
    }
   ],
   "source": [
    "data1_oz = data1[0] - data1[1]\n",
    "data1_o1 = data1[2] - data1[1]\n",
    "data1_o2 = data1[3] - data1[1]\n",
    "print(len(data1_o1))\n",
    "data2_oz = data2[0] - data2[1]\n",
    "data2_o1 = data2[2] - data2[1]\n",
    "data2_o2 = data2[3] - data2[1]\n",
    "print(len(data2_o1))\n",
    "data3_oz = data3[0] - data3[1]\n",
    "data3_o1 = data3[2] - data3[1]\n",
    "data3_o2 = data3[3] - data3[1]\n",
    "print(len(data3_o1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlapping_sets(data, set_size=500, overlap_fraction=0.5):\n",
    "    step = int(set_size * (1 - overlap_fraction))\n",
    "    sets = []\n",
    "    for i in range(0, len(data) - set_size + 1, step):\n",
    "        sets.append(data[i:i + set_size])\n",
    "    return sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_set_oz = create_overlapping_sets(data1_oz, set_size=1000, overlap_fraction=0.5)\n",
    "data1_set_o1 = create_overlapping_sets(data1_o1, set_size=1000, overlap_fraction=0.5)\n",
    "data1_set_o2 = create_overlapping_sets(data1_o2, set_size=1000, overlap_fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_set_oz = create_overlapping_sets(data2_oz, set_size=1000, overlap_fraction=0.5)\n",
    "data2_set_o1 = create_overlapping_sets(data2_o1, set_size=1000, overlap_fraction=0.5)\n",
    "data2_set_o2 = create_overlapping_sets(data2_o2, set_size=1000, overlap_fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_set_oz = create_overlapping_sets(data3_oz, set_size=1000, overlap_fraction=0.5)\n",
    "data3_set_o1 = create_overlapping_sets(data3_o1, set_size=1000, overlap_fraction=0.5)\n",
    "data3_set_o2 = create_overlapping_sets(data3_o2, set_size=1000, overlap_fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "327\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "print(len(data1_set_oz))\n",
    "print(len(data2_set_oz))\n",
    "print(len(data3_set_oz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, Pxx = welch(data_oz, fs=250, nperseg= 250*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_fft_oz = []\n",
    "data1_fft_o1 = []\n",
    "data1_fft_o2 = []\n",
    "for i in range(len(data1_set_oz)):\n",
    "    f, Pxx = welch(data1_set_oz[i], fs=250, nperseg= 250*4)\n",
    "    data1_fft_oz.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data1_set_o1[i], fs=250, nperseg= 250*4)\n",
    "    data1_fft_o1.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data1_set_o2[i], fs=250, nperseg= 250*4)\n",
    "    data1_fft_o2.append(Pxx[0:121])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25  2.5   2.75\n",
      "  3.    3.25  3.5   3.75  4.    4.25  4.5   4.75  5.    5.25  5.5   5.75\n",
      "  6.    6.25  6.5   6.75  7.    7.25  7.5   7.75  8.    8.25  8.5   8.75\n",
      "  9.    9.25  9.5   9.75 10.   10.25 10.5  10.75 11.   11.25 11.5  11.75\n",
      " 12.   12.25 12.5  12.75 13.   13.25 13.5  13.75 14.   14.25 14.5  14.75\n",
      " 15.   15.25 15.5  15.75 16.   16.25 16.5  16.75 17.   17.25 17.5  17.75\n",
      " 18.   18.25 18.5  18.75 19.   19.25 19.5  19.75 20.   20.25 20.5  20.75\n",
      " 21.   21.25 21.5  21.75 22.   22.25 22.5  22.75 23.   23.25 23.5  23.75\n",
      " 24.   24.25 24.5  24.75 25.   25.25 25.5  25.75 26.   26.25 26.5  26.75\n",
      " 27.   27.25 27.5  27.75 28.   28.25 28.5  28.75 29.   29.25 29.5  29.75\n",
      " 30.  ]\n"
     ]
    }
   ],
   "source": [
    "print(f[0:121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_fft_oz = []\n",
    "data2_fft_o1 = []\n",
    "data2_fft_o2 = []\n",
    "for i in range(len(data2_set_oz)):\n",
    "    f, Pxx = welch(data2_set_oz[i], fs=250, nperseg= 250*4)\n",
    "    data2_fft_oz.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data2_set_o1[i], fs=250, nperseg= 250*4)\n",
    "    data2_fft_o1.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data2_set_o2[i], fs=250, nperseg= 250*4)\n",
    "    data2_fft_o2.append(Pxx[0:121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_fft_oz = []\n",
    "data3_fft_o1 = []\n",
    "data3_fft_o2 = []\n",
    "for i in range(len(data3_set_oz)):\n",
    "    f, Pxx = welch(data3_set_oz[i], fs=250, nperseg= 250*4)\n",
    "    data3_fft_oz.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data3_set_o1[i], fs=250, nperseg= 250*4)\n",
    "    data3_fft_o1.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data3_set_o2[i], fs=250, nperseg= 250*4)\n",
    "    data3_fft_o2.append(Pxx[0:121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3_fft_oz = np.array(data3_fft_oz)\n",
    "# data3_fft_o1 = np.array(data3_fft_o1)\n",
    "# data3_fft_o2 = np.array(data3_fft_o2)\n",
    "# print(data3_fft_o2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(889, 363)\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "# รวมข้อมูลจากทุก feature สำหรับแต่ละ class และ flatten ข้อมูล\n",
    "combined_data1 = np.hstack((data1_fft_oz, data1_fft_o1, data1_fft_o2))\n",
    "combined_data2 = np.hstack((data2_fft_oz, data2_fft_o1, data2_fft_o2))\n",
    "combined_data3 = np.hstack((data3_fft_oz, data3_fft_o1, data3_fft_o2))\n",
    "\n",
    "# รวมข้อมูลจากทุก class เข้าด้วยกัน\n",
    "combined_data = np.vstack((combined_data1, combined_data2, combined_data3))\n",
    "\n",
    "# ตรวจสอบว่าข้อมูลมีขนาดที่ถูกต้อง\n",
    "print(combined_data.shape)  # ควรได้ (จำนวน samples ทั้งหมด, จำนวน features)\n",
    "\n",
    "# สร้าง label สำหรับแต่ละ class\n",
    "labels = np.array([0]*len(data1_fft_oz) + [1]*len(data2_fft_oz) + [2]*len(data3_fft_oz))\n",
    "\n",
    "# # ทำ UMAP\n",
    "# reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors = 50)\n",
    "# embedding = reducer.fit_transform(combined_data)\n",
    "\n",
    "# # แสดงผลการลดมิติ\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# scatter = plt.scatter(embedding[:, 0], embedding[:, 1], c=labels, cmap='Spectral', s=5)\n",
    "# plt.legend(handles=scatter.legend_elements()[0], labels=['Class 1', 'Class 2', 'Class 3'])\n",
    "# plt.title('UMAP projection of the dataset')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(combined_data.shape)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.signal import welch, find_peaks, butter, filtfilt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cross_decomposition import CCA\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "# # Define function to calculate PSD\n",
    "# def calculate_psd(data, fs=250, nperseg=250*4):\n",
    "#     f, Pxx = welch(data, fs=fs, nperseg=nperseg)\n",
    "#     return f, Pxx\n",
    "\n",
    "# # Define function to detect peaks\n",
    "# def detect_peaks(Pxx):\n",
    "#     peaks, _ = find_peaks(Pxx)\n",
    "#     return peaks\n",
    "\n",
    "# # Define function for band-pass filtering\n",
    "# def band_pass_filter(data, lowcut, highcut, fs=250, order=5):\n",
    "#     nyquist = 0.5 * fs\n",
    "#     low = lowcut / nyquist\n",
    "#     high = highcut / nyquist\n",
    "#     b, a = butter(order, [low, high], btype='band')\n",
    "#     y = filtfilt(b, a, data)\n",
    "#     return y\n",
    "\n",
    "# # Apply band-pass filter and calculate PSD for each dataset\n",
    "# def process_data(data_set):\n",
    "#     filtered_data = [band_pass_filter(epoch, 0.5, 50) for epoch in data_set]\n",
    "#     data_fft = []\n",
    "#     for epoch in filtered_data:\n",
    "#         f, Pxx = calculate_psd(epoch)\n",
    "#         peaks = detect_peaks(Pxx)\n",
    "#         data_fft.append(Pxx[:121])\n",
    "#     return data_fft\n",
    "\n",
    "# # Process each dataset\n",
    "# data1_fft_oz = process_data(data1_set_oz)\n",
    "# data1_fft_o1 = process_data(data1_set_o1)\n",
    "# data1_fft_o2 = process_data(data1_set_o2)\n",
    "\n",
    "# data2_fft_oz = process_data(data2_set_oz)\n",
    "# data2_fft_o1 = process_data(data2_set_o1)\n",
    "# data2_fft_o2 = process_data(data2_set_o2)\n",
    "\n",
    "# data3_fft_oz = process_data(data3_set_oz)\n",
    "# data3_fft_o1 = process_data(data3_set_o1)\n",
    "# data3_fft_o2 = process_data(data3_set_o2)\n",
    "\n",
    "# # Combine the data\n",
    "# combined_data1 = np.hstack((data1_fft_oz, data1_fft_o1, data1_fft_o2))\n",
    "# combined_data2 = np.hstack((data2_fft_oz, data2_fft_o1, data2_fft_o2))\n",
    "# combined_data3 = np.hstack((data3_fft_oz, data3_fft_o1, data3_fft_o2))\n",
    "\n",
    "# # Combine all data\n",
    "# combined_data = np.vstack((combined_data1, combined_data2, combined_data3))\n",
    "\n",
    "# # Create labels for each class\n",
    "# labels = np.array([0]*len(data1_fft_oz) + [1]*len(data2_fft_oz) + [2]*len(data3_fft_oz))\n",
    "\n",
    "# # Check that the combined data and labels have the correct shape\n",
    "# print(f\"Combined data shape: {combined_data.shape}\")\n",
    "# print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# # Canonical Correlation Analysis (CCA)\n",
    "# cca = CCA(n_components=1)  \n",
    "# combined_data_cca = cca.fit_transform(combined_data, labels)\n",
    "\n",
    "# # For each array in the tuple, access and perform a flat transformation.\n",
    "# combined_data_flat = np.hstack([arr.reshape(-1, 1) for arr in combined_data_cca])\n",
    "\n",
    "# # Features Normalization\n",
    "# scaler = StandardScaler()\n",
    "# X_normalized = scaler.fit_transform(combined_data_flat)\n",
    "\n",
    "# # Features Classification using Random Forest\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_normalized, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Process the results obtained from the test.\n",
    "# y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# # คำนวณ precision, recall, F1-score และ support สำหรับแต่ละคลาส\n",
    "# precision, recall, f1_score, support = precision_recall_fscore_support(y_test, y_pred_rf)\n",
    "\n",
    "# # แสดงผล precision, recall, F1-score และ support สำหรับแต่ละคลาส\n",
    "# for i in range(len(precision)):\n",
    "#     print(f\"Class {i}: Precision={precision[i]}, Recall={recall[i]}, F1-score={f1_score[i]}, Support={support[i]}\")\n",
    "\n",
    "# # แสดง classification report ที่ประมวลผลจากการทดสอบ\n",
    "# print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# # แสดง confusion matrix ที่ประมวลผลจากการทดสอบ\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   0.981481  0.946429  0.963636  0.981481\n",
      "1          1.0   0.985294  0.957143  0.971014  0.985294\n",
      "2          2.0   0.928571  1.000000  0.962963  0.928571\n",
      "average    NaN   0.967524  0.966292  0.966341  0.966292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/random_forest_model.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# แบ่งข้อมูลเป็น train set และ test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# สร้างและฝึกโมเดล Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "precision_lda_per_class = precision_score(y_test, y_pred_rf, average=None)\n",
    "recall_lda_per_class = recall_score(y_test, y_pred_rf, average=None)\n",
    "f1_lda_per_class = f1_score(y_test, y_pred_rf, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_lda_per_class = []\n",
    "for class_label in range(len(precision_lda_per_class)):\n",
    "    correct_predictions = ((y_pred_rf == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_rf == class_label).sum()\n",
    "    # print(y_pred_rf)\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_lda_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_lda = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_lda = precision_score(y_test, y_pred_rf, average='weighted')\n",
    "avg_recall_lda = recall_score(y_test, y_pred_rf, average='weighted')\n",
    "avg_f1_lda = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_lda_per_class)),\n",
    "    'Precision': precision_lda_per_class,\n",
    "    'Recall': recall_lda_per_class,\n",
    "    'F1-score': f1_lda_per_class,\n",
    "    'Accuracy': accuracy_lda_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_df.loc['average'] = [None, avg_precision_lda, avg_recall_lda, avg_f1_lda, avg_accuracy_lda]\n",
    "\n",
    "print(results_df)\n",
    "dump(rf_classifier, 'model/random_forest_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   0.774194  0.857143  0.813559  0.774194\n",
      "1          1.0   0.876923  0.814286  0.844444  0.876923\n",
      "2          2.0   1.000000  0.980769  0.990291  1.000000\n",
      "average    NaN   0.880559  0.876404  0.877335  0.876404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/svm_model.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# แบ่งข้อมูลเป็น train set และ test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# สร้างและฝึกโมเดล SVM 'poly' 'linear' 'rbf'\n",
    "svm_classifier = SVC(C=0.1, gamma=1, kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_lda_per_class = precision_score(y_test, y_pred_svm, average=None)\n",
    "recall_lda_per_class = recall_score(y_test, y_pred_svm, average=None)\n",
    "f1_lda_per_class = f1_score(y_test, y_pred_svm, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_lda_per_class = []\n",
    "for class_label in range(len(precision_lda_per_class)):\n",
    "    correct_predictions = ((y_pred_svm == class_label) & (y_test == class_label)).sum()\n",
    "    # print(y_test)\n",
    "    total_predictions = (y_pred_svm == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_lda_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_lda = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_lda = precision_score(y_test, y_pred_svm, average='weighted')\n",
    "avg_recall_lda = recall_score(y_test, y_pred_svm, average='weighted')\n",
    "avg_f1_lda = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_lda_per_class)),\n",
    "    'Precision': precision_lda_per_class,\n",
    "    'Recall': recall_lda_per_class,\n",
    "    'F1-score': f1_lda_per_class,\n",
    "    'Accuracy': accuracy_lda_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_df.loc['average'] = [None, avg_precision_lda, avg_recall_lda, avg_f1_lda, avg_accuracy_lda]\n",
    "\n",
    "print(results_df)\n",
    "dump(svm_classifier, 'model/svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # แบ่งข้อมูลเป็น train set และ test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # กำหนดช่วงของ hyperparameters ที่ต้องการทดสอบ\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],  # regularization parameter\n",
    "#     'gamma': [1, 0.1, 0.01, 0.001],  # kernel coefficient\n",
    "#     'kernel': ['rbf', 'linear', 'poly']  # kernel function\n",
    "# }\n",
    "\n",
    "# # สร้าง GridSearchCV object\n",
    "# grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# # ฝึกโมเดล\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # แสดง hyperparameters ที่ดีที่สุด\n",
    "# print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# # ทำนายบน test set\n",
    "# y_pred_svm = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# # ประเมินประสิทธิภาพของโมเดล SVM ที่ปรับ hyperparameters แล้ว\n",
    "# accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "# print(\"SVM Accuracy:\", accuracy_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   1.000000  0.964286  0.981818  1.000000\n",
      "1          1.0   0.971429  0.971429  0.971429  0.971429\n",
      "2          2.0   0.962963  1.000000  0.981132  0.962963\n",
      "average    NaN   0.977944  0.977528  0.977532  0.977528\n",
      "SVM Confusion Matrix:\n",
      "[[54  2  0]\n",
      " [ 0 68  2]\n",
      " [ 0  0 52]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/lda_model.joblib']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# แบ่งข้อมูลเป็น train set และ test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# สร้างและฝึกโมเดล LDA\n",
    "lda_classifier = LinearDiscriminantAnalysis()\n",
    "lda_classifier.fit(X_train, y_train)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_lda = lda_classifier.predict(X_test)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_lda_per_class = precision_score(y_test, y_pred_lda, average=None)\n",
    "recall_lda_per_class = recall_score(y_test, y_pred_lda, average=None)\n",
    "f1_lda_per_class = f1_score(y_test, y_pred_lda, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_lda_per_class = []\n",
    "for class_label in range(len(precision_lda_per_class)):\n",
    "    correct_predictions = ((y_pred_lda == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_lda == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_lda_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_lda = accuracy_score(y_test, y_pred_lda)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_lda = precision_score(y_test, y_pred_lda, average='weighted')\n",
    "avg_recall_lda = recall_score(y_test, y_pred_lda, average='weighted')\n",
    "avg_f1_lda = f1_score(y_test, y_pred_lda, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_lda_per_class)),\n",
    "    'Precision': precision_lda_per_class,\n",
    "    'Recall': recall_lda_per_class,\n",
    "    'F1-score': f1_lda_per_class,\n",
    "    'Accuracy': accuracy_lda_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_df.loc['average'] = [None, avg_precision_lda, avg_recall_lda, avg_f1_lda, avg_accuracy_lda]\n",
    "\n",
    "print(results_df)\n",
    "conf_matrix_svm = confusion_matrix(y_test, y_pred_lda)\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(conf_matrix_svm)\n",
    "dump(lda_classifier, 'model/lda_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   0.784314  0.714286  0.747664  0.784314\n",
      "1          1.0   0.777778  0.800000  0.788732  0.777778\n",
      "2          2.0   0.927273  0.980769  0.953271  0.927273\n",
      "average    NaN   0.823507  0.825843  0.823879  0.825843\n",
      "KNN Confusion Matrix:\n",
      "[[40 16  0]\n",
      " [10 56  4]\n",
      " [ 1  0 51]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/knn_model.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# สร้างและฝึกโมเดล KNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_knn = knn_classifier.predict(X_test)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_knn_per_class = precision_score(y_test, y_pred_knn, average=None)\n",
    "recall_knn_per_class = recall_score(y_test, y_pred_knn, average=None)\n",
    "f1_knn_per_class = f1_score(y_test, y_pred_knn, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_knn_per_class = []\n",
    "for class_label in range(len(precision_knn_per_class)):\n",
    "    correct_predictions = ((y_pred_knn == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_knn == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_knn_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_knn = precision_score(y_test, y_pred_knn, average='weighted')\n",
    "avg_recall_knn = recall_score(y_test, y_pred_knn, average='weighted')\n",
    "avg_f1_knn = f1_score(y_test, y_pred_knn, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_knn_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_knn_per_class)),\n",
    "    'Precision': precision_knn_per_class,\n",
    "    'Recall': recall_knn_per_class,\n",
    "    'F1-score': f1_knn_per_class,\n",
    "    'Accuracy': accuracy_knn_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_knn_df.loc['average'] = [None, avg_precision_knn, avg_recall_knn, avg_f1_knn, avg_accuracy_knn]\n",
    "\n",
    "print(results_knn_df)\n",
    "conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "print(\"KNN Confusion Matrix:\")\n",
    "print(conf_matrix_knn)\n",
    "dump(knn_classifier, 'model/knn_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5732 - loss: 160986272.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7073 - loss: 596.0427 \n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8149 - loss: 344.1021 \n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 727us/step - accuracy: 0.7739 - loss: 124.1787\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8462 - loss: 85.5144 \n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8160 - loss: 95.3581 \n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8708 - loss: 71.1046  \n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8699 - loss: 68.6268 \n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 941us/step - accuracy: 0.8729 - loss: 44.4801\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8920 - loss: 40.0791 \n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8843 - loss: 45.6979\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9035 - loss: 36.3228 \n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9252 - loss: 17.5786 \n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9415 - loss: 9.5903 \n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9521 - loss: 6.5674 \n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9045 - loss: 36.9581    \n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9020 - loss: 26.8414\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9322 - loss: 21.2890 \n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9478 - loss: 10.6534\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9720 - loss: 4.7790 \n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716us/step - accuracy: 0.9541 - loss: 6.7471\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9264 - loss: 11.2342 \n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9598 - loss: 4.3734 \n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9657 - loss: 3.3875     \n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9448 - loss: 13.9409\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9026 - loss: 41.9486 \n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9073 - loss: 28.2219    \n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9307 - loss: 26.7190    \n",
      "Epoch 29/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9418 - loss: 9.2387  \n",
      "Epoch 30/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607us/step - accuracy: 0.8871 - loss: 52.3215\n",
      "Epoch 31/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9594 - loss: 6.6670  \n",
      "Epoch 32/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 929us/step - accuracy: 0.9154 - loss: 21.9442\n",
      "Epoch 33/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9744 - loss: 3.9022     \n",
      "Epoch 34/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - accuracy: 0.9490 - loss: 9.7880\n",
      "Epoch 35/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9686 - loss: 10.6251 \n",
      "Epoch 36/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9898 - loss: 2.0764     \n",
      "Epoch 37/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9840 - loss: 1.9620 \n",
      "Epoch 38/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 757us/step - accuracy: 0.9747 - loss: 6.4283   \n",
      "Epoch 39/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9575 - loss: 8.7927 \n",
      "Epoch 40/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9785 - loss: 2.6595     \n",
      "Epoch 41/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9753 - loss: 3.6451 \n",
      "Epoch 42/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9887 - loss: 0.8157     \n",
      "Epoch 43/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9835 - loss: 1.9570 \n",
      "Epoch 44/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9770 - loss: 2.8142 \n",
      "Epoch 45/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9918 - loss: 0.7551     \n",
      "Epoch 46/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9510 - loss: 14.7319    \n",
      "Epoch 47/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9801 - loss: 2.8714  \n",
      "Epoch 48/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.2113     \n",
      "Epoch 49/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713us/step - accuracy: 0.9957 - loss: 0.7520   \n",
      "Epoch 50/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9729 - loss: 6.8128  \n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n",
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   0.842105  0.857143  0.849558  0.842105\n",
      "1          1.0   0.869565  0.857143  0.863309  0.869565\n",
      "2          2.0   0.980769  0.980769  0.980769  0.980769\n",
      "average    NaN   0.893413  0.893258  0.893297  0.893258\n",
      "ANN Confusion Matrix:\n",
      "[[48  8  0]\n",
      " [ 9 60  1]\n",
      " [ 0  1 51]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# แปลง y_train และ y_test ให้เป็นแบบ one-hot encoding\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# สร้างโมเดล ANN แบบ Feed-Forward\n",
    "ann_model = Sequential()\n",
    "ann_model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "ann_model.add(Dense(64, activation='relu'))\n",
    "ann_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# คอมไพล์โมเดล\n",
    "ann_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# ฝึกโมเดล\n",
    "ann_model.fit(X_train, y_train_one_hot, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_ann = np.argmax(ann_model.predict(X_test), axis=1)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_ann_per_class = precision_score(y_test, y_pred_ann, average=None)\n",
    "recall_ann_per_class = recall_score(y_test, y_pred_ann, average=None)\n",
    "f1_ann_per_class = f1_score(y_test, y_pred_ann, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_ann_per_class = []\n",
    "for class_label in range(len(precision_ann_per_class)):\n",
    "    correct_predictions = ((y_pred_ann == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_ann == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_ann_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_ann = accuracy_score(y_test, y_pred_ann)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_ann = precision_score(y_test, y_pred_ann, average='weighted')\n",
    "avg_recall_ann = recall_score(y_test, y_pred_ann, average='weighted')\n",
    "avg_f1_ann = f1_score(y_test, y_pred_ann, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_ann_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_ann_per_class)),\n",
    "    'Precision': precision_ann_per_class,\n",
    "    'Recall': recall_ann_per_class,\n",
    "    'F1-score': f1_ann_per_class,\n",
    "    'Accuracy': accuracy_ann_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_ann_df.loc['average'] = [None, avg_precision_ann, avg_recall_ann, avg_f1_ann, avg_accuracy_ann]\n",
    "\n",
    "print(results_ann_df)\n",
    "conf_matrix_ann = confusion_matrix(y_test, y_pred_ann)\n",
    "print(\"ANN Confusion Matrix:\")\n",
    "print(conf_matrix_ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.7531 - loss: 0.8760\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9319 - loss: 0.6338\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9603 - loss: 0.1410\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9839 - loss: 0.0988\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9799 - loss: 0.0924\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9805 - loss: 0.0801\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9901 - loss: 0.0487\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9927 - loss: 0.0429\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9900 - loss: 0.0449\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9803 - loss: 0.0616\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9959 - loss: 0.0267\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9795 - loss: 0.0760\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9874 - loss: 0.0407\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9909 - loss: 0.0391\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9786 - loss: 0.0752\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9733 - loss: 0.4110\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9815 - loss: 0.0753\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9591 - loss: 0.1061\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9852 - loss: 0.0540\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9940 - loss: 0.0305\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9900 - loss: 0.0344\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9902 - loss: 0.0285\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9935 - loss: 0.0257\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9931 - loss: 0.0302\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9908 - loss: 0.0324\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9869 - loss: 0.0301\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9949 - loss: 0.0197\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9975 - loss: 0.0076\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9990 - loss: 0.0101\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 0.0076\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9987 - loss: 0.0066\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9951 - loss: 0.0106\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9946 - loss: 0.0127\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9999 - loss: 0.0047\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9802 - loss: 0.0449\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9996 - loss: 0.0083\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0052\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0050\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9964 - loss: 0.0078\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9997 - loss: 0.0015  \n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9994 - loss: 0.0038\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9970 - loss: 0.0084\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9998 - loss: 0.0016  \n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0013\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0016\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9995 - loss: 0.0032\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 9.9593e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0027\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9995 - loss: 0.0018\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9993 - loss: 0.0017  \n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9951 - loss: 0.0114\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9989 - loss: 0.0066\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 9.3520e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9975 - loss: 0.0030\n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 8.0025e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0028\n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 6.2266e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 8.6683e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 9.0918e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9997 - loss: 0.0014\n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 8.0094e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 7.2106e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 2.4187e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 3.2741e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 79/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 7.2824e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 5.4732e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 8.0931e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 5.3650e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0037\n",
      "Epoch 84/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 4.6787e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0011\n",
      "Epoch 86/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 9.9377e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 4.4518e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 7.6790e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 6.1524e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 1.6636e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 2.6139e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 3.6125e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 1.2980e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 95/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 3.4196e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 8.1709e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 6.3747e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 99/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9985 - loss: 0.0037\n",
      "Epoch 100/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9976 - loss: 0.0059\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   0.901639  0.982143  0.940171  0.901639\n",
      "1          1.0   1.000000  0.971429  0.985507  1.000000\n",
      "2          2.0   0.959184  0.903846  0.930693  0.959184\n",
      "average    NaN   0.957131  0.955056  0.955231  0.955056\n",
      "CNN Confusion Matrix:\n",
      "[[55  0  1]\n",
      " [ 1 68  1]\n",
      " [ 5  0 47]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dense, ReLU, Softmax, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# แบ่งข้อมูลเป็น train set และ test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize ข้อมูล\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ปรับข้อมูลให้เป็นรูปแบบ 3D สำหรับ Conv1D\n",
    "X_train_reshaped = X_train.reshape(-1, 363, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 363, 1)\n",
    "\n",
    "# สร้างโมเดล CNN สำหรับข้อมูล 1D\n",
    "model = Sequential()\n",
    "\n",
    "# Convolution Layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, input_shape=(363, 1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(ReLU())\n",
    "\n",
    "# เพิ่มอีก Convolution Layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(ReLU())\n",
    "\n",
    "# Dense Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256))\n",
    "model.add(ReLU())\n",
    "\n",
    "# Output Layer\n",
    "num_classes = len(np.unique(y_train))\n",
    "model.add(Dense(units=num_classes))\n",
    "model.add(Softmax())\n",
    "\n",
    "# คอมไพล์โมเดล\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# แปลง y_train และ y_test ให้เป็นแบบ one-hot encoding\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# ฝึกโมเดล\n",
    "model.fit(X_train_reshaped, y_train_one_hot, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_cnn = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_cnn_per_class = precision_score(y_test, y_pred_cnn, average=None)\n",
    "recall_cnn_per_class = recall_score(y_test, y_pred_cnn, average=None)\n",
    "f1_cnn_per_class = f1_score(y_test, y_pred_cnn, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_cnn_per_class = []\n",
    "for class_label in range(len(precision_cnn_per_class)):\n",
    "    correct_predictions = ((y_pred_cnn == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_cnn == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_cnn_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_cnn = accuracy_score(y_test, y_pred_cnn)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_cnn = precision_score(y_test, y_pred_cnn, average='weighted')\n",
    "avg_recall_cnn = recall_score(y_test, y_pred_cnn, average='weighted')\n",
    "avg_f1_cnn = f1_score(y_test, y_pred_cnn, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_cnn_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_cnn_per_class)),\n",
    "    'Precision': precision_cnn_per_class,\n",
    "    'Recall': recall_cnn_per_class,\n",
    "    'F1-score': f1_cnn_per_class,\n",
    "    'Accuracy': accuracy_cnn_per_class\n",
    "})\n",
    "\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_cnn_df.loc['average'] = [None, avg_precision_cnn, avg_recall_cnn, avg_f1_cnn, avg_accuracy_cnn]\n",
    "\n",
    "print(results_cnn_df)\n",
    "conf_matrix_cnn = confusion_matrix(y_test, y_pred_cnn)\n",
    "print(\"CNN Confusion Matrix:\")\n",
    "print(conf_matrix_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 181ms/step - accuracy: 0.3834 - loss: 1.1056 - val_accuracy: 0.3933 - val_loss: 1.1008 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.3901 - loss: 1.1019 - val_accuracy: 0.3933 - val_loss: 1.0974 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.3527 - loss: 1.1009 - val_accuracy: 0.3933 - val_loss: 1.0928 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.3981 - loss: 1.0981 - val_accuracy: 0.3933 - val_loss: 1.0889 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 163ms/step - accuracy: 0.3632 - loss: 1.0999 - val_accuracy: 0.3933 - val_loss: 1.0869 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.3771 - loss: 1.0964 - val_accuracy: 0.3933 - val_loss: 1.0858 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 164ms/step - accuracy: 0.3594 - loss: 1.0959 - val_accuracy: 0.3933 - val_loss: 1.0844 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.3594 - loss: 1.0972 - val_accuracy: 0.3933 - val_loss: 1.0841 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.3810 - loss: 1.0952 - val_accuracy: 0.3933 - val_loss: 1.0844 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.3533 - loss: 1.0961 - val_accuracy: 0.3933 - val_loss: 1.0837 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.3962 - loss: 1.0920 - val_accuracy: 0.3933 - val_loss: 1.0831 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.3690 - loss: 1.0922 - val_accuracy: 0.3933 - val_loss: 1.0843 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.3860 - loss: 1.1008 - val_accuracy: 0.3933 - val_loss: 1.0057 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.4304 - loss: 0.9725 - val_accuracy: 0.5169 - val_loss: 0.9498 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.5089 - loss: 0.9726 - val_accuracy: 0.5169 - val_loss: 0.9417 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5042 - loss: 0.9573 - val_accuracy: 0.5112 - val_loss: 0.9513 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.4835 - loss: 0.9548 - val_accuracy: 0.5169 - val_loss: 0.9403 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.4970 - loss: 0.9427 - val_accuracy: 0.5169 - val_loss: 0.9363 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.5239 - loss: 0.9286 - val_accuracy: 0.5169 - val_loss: 0.9376 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.4824 - loss: 0.9488 - val_accuracy: 0.5169 - val_loss: 0.9349 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4925 - loss: 0.9662 - val_accuracy: 0.5112 - val_loss: 0.9436 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5040 - loss: 0.9528 - val_accuracy: 0.5169 - val_loss: 0.9374 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.5311 - loss: 0.9432 - val_accuracy: 0.5112 - val_loss: 0.9400 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.5209 - loss: 0.9410 - val_accuracy: 0.5169 - val_loss: 0.9339 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5098 - loss: 0.9417 - val_accuracy: 0.5112 - val_loss: 0.9415 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.4981 - loss: 0.9431 - val_accuracy: 0.5112 - val_loss: 0.9411 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.5000 - loss: 0.9575 - val_accuracy: 0.5112 - val_loss: 0.9393 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - accuracy: 0.5295 - loss: 0.9378 - val_accuracy: 0.5169 - val_loss: 0.9340 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.5052 - loss: 0.9391 - val_accuracy: 0.5112 - val_loss: 0.9402 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4811 - loss: 0.9694 - val_accuracy: 0.5112 - val_loss: 0.9375 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5268 - loss: 0.9403 - val_accuracy: 0.5112 - val_loss: 0.9427 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.4970 - loss: 0.9539 - val_accuracy: 0.5112 - val_loss: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5263 - loss: 0.9679 - val_accuracy: 0.5112 - val_loss: 0.9415 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.5162 - loss: 0.9628\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.5155 - loss: 0.9626 - val_accuracy: 0.5056 - val_loss: 0.9506 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.4974 - loss: 0.9530 - val_accuracy: 0.5056 - val_loss: 0.9503 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.4766 - loss: 0.9824 - val_accuracy: 0.5056 - val_loss: 0.9499 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.4825 - loss: 0.9728 - val_accuracy: 0.5056 - val_loss: 0.9508 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.4855 - loss: 0.9640 - val_accuracy: 0.5056 - val_loss: 0.9506 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.4940 - loss: 0.9784 - val_accuracy: 0.5056 - val_loss: 0.9500 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5175 - loss: 0.9562 - val_accuracy: 0.5056 - val_loss: 0.9496 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.5039 - loss: 0.9609 - val_accuracy: 0.5056 - val_loss: 0.9493 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.4822 - loss: 0.9642 - val_accuracy: 0.5056 - val_loss: 0.9489 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.4731 - loss: 0.9803 - val_accuracy: 0.5056 - val_loss: 0.9459 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.4876 - loss: 0.9580\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.4880 - loss: 0.9580 - val_accuracy: 0.5056 - val_loss: 0.9457 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 144ms/step - accuracy: 0.4862 - loss: 0.9783 - val_accuracy: 0.5056 - val_loss: 0.9457 - learning_rate: 1.0000e-05\n",
      "Epoch 46/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 0.4840 - loss: 0.9696 - val_accuracy: 0.5056 - val_loss: 0.9457 - learning_rate: 1.0000e-05\n",
      "Epoch 47/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.4891 - loss: 0.9696 - val_accuracy: 0.5056 - val_loss: 0.9457 - learning_rate: 1.0000e-05\n",
      "Epoch 48/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.4908 - loss: 0.9683 - val_accuracy: 0.5056 - val_loss: 0.9457 - learning_rate: 1.0000e-05\n",
      "Epoch 49/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4937 - loss: 0.9605 - val_accuracy: 0.5056 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5063 - loss: 0.9574 - val_accuracy: 0.5056 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.5301 - loss: 0.9482 - val_accuracy: 0.5056 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.4714 - loss: 0.9682 - val_accuracy: 0.5056 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.5077 - loss: 0.9566 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5222 - loss: 0.9307\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.5212 - loss: 0.9320 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.4825 - loss: 0.9726 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.5023 - loss: 0.9529 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.5162 - loss: 0.9519 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 58/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.4623 - loss: 0.9607 - val_accuracy: 0.5056 - val_loss: 0.9456 - learning_rate: 1.0000e-05\n",
      "Epoch 59/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5098 - loss: 0.9522 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 60/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 0.4985 - loss: 0.9686 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 61/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.4841 - loss: 0.9731 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 62/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.4789 - loss: 0.9678 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 63/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.5167 - loss: 0.9423 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 64/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4929 - loss: 0.9754 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 65/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4984 - loss: 0.9658 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 66/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.4988 - loss: 0.9537 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.4889 - loss: 0.9873 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.4759 - loss: 0.9704 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.4838 - loss: 0.9739 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5165 - loss: 0.9558 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.5037 - loss: 0.9508 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4973 - loss: 0.9581 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4939 - loss: 0.9757 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.4838 - loss: 0.9736 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.4978 - loss: 0.9699 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5086 - loss: 0.9606 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.4796 - loss: 0.9659 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.4794 - loss: 0.9658 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5000 - loss: 0.9528 - val_accuracy: 0.5056 - val_loss: 0.9484 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.4679 - loss: 0.9582 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.5068 - loss: 0.9592 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4758 - loss: 0.9704 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5363 - loss: 0.9339 - val_accuracy: 0.5056 - val_loss: 0.9483 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.4923 - loss: 0.9649 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5100 - loss: 0.9445 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.5047 - loss: 0.9582 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.4821 - loss: 0.9744 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.5160 - loss: 0.9474 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.5079 - loss: 0.9628 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.5073 - loss: 0.9640 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.4841 - loss: 0.9686 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.4899 - loss: 0.9550 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5142 - loss: 0.9403 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.4872 - loss: 0.9632 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.4751 - loss: 0.9674 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.4996 - loss: 0.9615 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.4964 - loss: 0.9702 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.5156 - loss: 0.9597 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.5073 - loss: 0.9556 - val_accuracy: 0.5056 - val_loss: 0.9482 - learning_rate: 1.0000e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.4787 - loss: 0.9587 - val_accuracy: 0.5056 - val_loss: 0.9481 - learning_rate: 1.0000e-05\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DCFA81C040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step\n",
      "         Class  Precision    Recall  F1-score  Accuracy\n",
      "0          0.0   0.000000  0.000000  0.000000  0.000000\n",
      "1          1.0   0.655172  0.542857  0.593750  0.655172\n",
      "2          2.0   0.433333  1.000000  0.604651  0.433333\n",
      "average    NaN   0.384244  0.505618  0.410137  0.505618\n",
      "LSTM Confusion Matrix:\n",
      "[[ 0 20 36]\n",
      " [ 0 38 32]\n",
      " [ 0  0 52]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toey\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Toey\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# แบ่งข้อมูลเป็น train set และ test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize ข้อมูล\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ปรับข้อมูลให้เป็นรูปแบบ 3D สำหรับ LSTM\n",
    "X_train_reshaped = X_train.reshape(-1, 363, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 363, 1)\n",
    "\n",
    "# สร้างโมเดล LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM Layer 1\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(363, 1), kernel_regularizer=l2(0.0001)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# LSTM Layer 2\n",
    "model.add(LSTM(70, return_sequences=False, kernel_regularizer=l2(0.0001)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(Dense(5, activation='relu'))\n",
    "\n",
    "# Output Layer with Softmax\n",
    "model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "# คอมไพล์โมเดล\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# แปลง y_train และ y_test ให้เป็นแบบ one-hot encoding\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=len(np.unique(y_train)))\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=len(np.unique(y_train)))\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# ฝึกโมเดล\n",
    "model.fit(X_train_reshaped, y_train_one_hot, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test_one_hot), callbacks=[reduce_lr], verbose=1)\n",
    "\n",
    "# ทำนายบน test set\n",
    "y_pred_lstm = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_lstm_per_class = precision_score(y_test, y_pred_lstm, average=None)\n",
    "recall_lstm_per_class = recall_score(y_test, y_pred_lstm, average=None)\n",
    "f1_lstm_per_class = f1_score(y_test, y_pred_lstm, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_lstm_per_class = []\n",
    "for class_label in range(len(precision_lstm_per_class)):\n",
    "    correct_predictions = ((y_pred_lstm == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_lstm == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_lstm_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_lstm = precision_score(y_test, y_pred_lstm, average='weighted')\n",
    "avg_recall_lstm = recall_score(y_test, y_pred_lstm, average='weighted')\n",
    "avg_f1_lstm = f1_score(y_test, y_pred_lstm, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_lstm_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_lstm_per_class)),\n",
    "    'Precision': precision_lstm_per_class,\n",
    "    'Recall': recall_lstm_per_class,\n",
    "    'F1-score': f1_lstm_per_class,\n",
    "    'Accuracy': accuracy_lstm_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_lstm_df.loc['average'] = [None, avg_precision_lstm, avg_recall_lstm, avg_f1_lstm, avg_accuracy_lstm]\n",
    "\n",
    "print(results_lstm_df)\n",
    "conf_matrix_lstm = confusion_matrix(y_test, y_pred_lstm)\n",
    "print(\"LSTM Confusion Matrix:\")\n",
    "print(conf_matrix_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toey\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 219ms/step - accuracy: 0.3646 - loss: 1.1201 - val_accuracy: 0.4157 - val_loss: 1.0651 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - accuracy: 0.4717 - loss: 1.0476 - val_accuracy: 0.5899 - val_loss: 0.8917 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.5327 - loss: 0.8894 - val_accuracy: 0.5730 - val_loss: 0.6854 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.6150 - loss: 0.7591 - val_accuracy: 0.6742 - val_loss: 0.8086 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 181ms/step - accuracy: 0.6263 - loss: 0.7932 - val_accuracy: 0.7640 - val_loss: 0.6659 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.6757 - loss: 0.7620 - val_accuracy: 0.8258 - val_loss: 0.6462 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.7150 - loss: 0.7290 - val_accuracy: 0.6517 - val_loss: 0.6835 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.6825 - loss: 0.7168 - val_accuracy: 0.6742 - val_loss: 0.6588 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - accuracy: 0.7136 - loss: 0.7172 - val_accuracy: 0.8146 - val_loss: 0.5971 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - accuracy: 0.7095 - loss: 0.6763 - val_accuracy: 0.7978 - val_loss: 0.6054 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - accuracy: 0.7384 - loss: 0.6762 - val_accuracy: 0.7528 - val_loss: 0.5449 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7341 - loss: 0.6296 - val_accuracy: 0.8202 - val_loss: 0.4692 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 188ms/step - accuracy: 0.7801 - loss: 0.5788 - val_accuracy: 0.7191 - val_loss: 0.5873 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 187ms/step - accuracy: 0.7717 - loss: 0.6082 - val_accuracy: 0.8202 - val_loss: 0.4740 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 185ms/step - accuracy: 0.7898 - loss: 0.5674 - val_accuracy: 0.7584 - val_loss: 0.6860 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.7782 - loss: 0.5952 - val_accuracy: 0.7978 - val_loss: 0.5051 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 205ms/step - accuracy: 0.7875 - loss: 0.5611 - val_accuracy: 0.7978 - val_loss: 0.5627 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 202ms/step - accuracy: 0.7754 - loss: 0.5842 - val_accuracy: 0.3202 - val_loss: 1.9653 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.3694 - loss: 1.4082 - val_accuracy: 0.3652 - val_loss: 1.1002 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 183ms/step - accuracy: 0.3650 - loss: 1.0997 - val_accuracy: 0.3652 - val_loss: 1.1015 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.3780 - loss: 1.0978 - val_accuracy: 0.3652 - val_loss: 1.1014 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.3429 - loss: 1.0996\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 200ms/step - accuracy: 0.3435 - loss: 1.0995 - val_accuracy: 0.3652 - val_loss: 1.1006 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 190ms/step - accuracy: 0.3485 - loss: 1.0958 - val_accuracy: 0.3652 - val_loss: 1.1006 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 189ms/step - accuracy: 0.3282 - loss: 1.0939 - val_accuracy: 0.3652 - val_loss: 1.1005 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m 1/23\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 187ms/step - accuracy: 0.3438 - loss: 1.1089"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# ฝึกโมเดล\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train_reshaped, y_train_one_hot, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test_reshaped, y_test_one_hot), callbacks\u001b[38;5;241m=\u001b[39m[reduce_lr, early_stopping], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/lstm_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# ทำนายบน test set\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# แบ่งข้อมูลเป็น train set และ test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_data, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Normalize ข้อมูล\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ปรับข้อมูลให้เป็นรูปแบบ 3D สำหรับ LSTM\n",
    "X_train_reshaped = X_train.reshape(-1, 363, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 363, 1)\n",
    "\n",
    "# สร้างโมเดล LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM Layer 1\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True, input_shape=(363, 1), kernel_regularizer=l2(0.0001))))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# LSTM Layer 2\n",
    "model.add(Bidirectional(LSTM(70, return_sequences=False, kernel_regularizer=l2(0.0001))))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(Dense(5, activation='relu'))\n",
    "\n",
    "# Output Layer with Softmax\n",
    "model.add(Dense(len(np.unique(y_train)), activation='softmax'))\n",
    "\n",
    "# คอมไพล์โมเดล\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# แปลง y_train และ y_test ให้เป็นแบบ one-hot encoding\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=len(np.unique(y_train)))\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=len(np.unique(y_train)))\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001, verbose=1)\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# ฝึกโมเดล\n",
    "model.fit(X_train_reshaped, y_train_one_hot, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test_one_hot), callbacks=[reduce_lr, early_stopping], verbose=1)\n",
    "model.save(\"model/lstm_model.h5\")\n",
    "# ทำนายบน test set\n",
    "y_pred_lstm = np.argmax(model.predict(X_test_reshaped), axis=1)\n",
    "\n",
    "# ประเมิน precision, recall, และ F1-score ของแต่ละคลาส\n",
    "precision_lstm_per_class = precision_score(y_test, y_pred_lstm, average=None)\n",
    "recall_lstm_per_class = recall_score(y_test, y_pred_lstm, average=None)\n",
    "f1_lstm_per_class = f1_score(y_test, y_pred_lstm, average=None)\n",
    "\n",
    "# คำนวณ accuracy ของแต่ละคลาส\n",
    "accuracy_lstm_per_class = []\n",
    "for class_label in range(len(precision_lstm_per_class)):\n",
    "    correct_predictions = ((y_pred_lstm == class_label) & (y_test == class_label)).sum()\n",
    "    total_predictions = (y_pred_lstm == class_label).sum()\n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    accuracy_lstm_per_class.append(accuracy)\n",
    "\n",
    "# คำนวณค่าเฉลี่ยของ accuracy\n",
    "avg_accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
    "\n",
    "# เฉลี่ย precision, recall, และ F1-score ของแต่ละคลาส\n",
    "avg_precision_lstm = precision_score(y_test, y_pred_lstm, average='weighted')\n",
    "avg_recall_lstm = recall_score(y_test, y_pred_lstm, average='weighted')\n",
    "avg_f1_lstm = f1_score(y_test, y_pred_lstm, average='weighted')\n",
    "\n",
    "# สร้าง DataFrame จากผลลัพธ์\n",
    "results_lstm_df = pd.DataFrame({\n",
    "    'Class': range(len(precision_lstm_per_class)),\n",
    "    'Precision': precision_lstm_per_class,\n",
    "    'Recall': recall_lstm_per_class,\n",
    "    'F1-score': f1_lstm_per_class,\n",
    "    'Accuracy': accuracy_lstm_per_class\n",
    "})\n",
    "\n",
    "# เพิ่มค่าเฉลี่ยของ accuracy และ precision, recall, F1-score ลงในตาราง\n",
    "results_lstm_df.loc['average'] = [None, avg_precision_lstm, avg_recall_lstm, avg_f1_lstm, avg_accuracy_lstm]\n",
    "\n",
    "print(results_lstm_df)\n",
    "conf_matrix_lstm = confusion_matrix(y_test, y_pred_lstm)\n",
    "print(\"LSTM Confusion Matrix:\")\n",
    "print(conf_matrix_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "streams_test1, header = pyxdf.load_xdf('../../../data_ssvep/Toey/SSVEP_data/test/6hz_10')\n",
    "raw_test1 = streams_test1[0][\"time_series\"].T #From Steam variable this query is EEG data\n",
    "\n",
    "streams_test2, header = pyxdf.load_xdf('../../../data_ssvep/Toey/SSVEP_data/test/20hz_10')\n",
    "raw_test2 = streams_test2[0][\"time_series\"].T #From Steam variable this query is EEG data\n",
    "\n",
    "streams_test3, header = pyxdf.load_xdf('../../../data_ssvep/Toey/SSVEP_data/test/0hz_10')\n",
    "raw_test3 = streams_test3[0][\"time_series\"].T #From Steam variable this query is EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 363)\n",
      "(83,)\n"
     ]
    }
   ],
   "source": [
    "data_test1 = raw_test1[0:4,:]\n",
    "data_test1_oz = data_test1[0] - data_test1[1]\n",
    "data_test1_o1 = data_test1[2] - data_test1[1]\n",
    "data_test1_o2 = data_test1[3] - data_test1[1]\n",
    "data_test1_set_oz = create_overlapping_sets(data_test1_oz, set_size=1000, overlap_fraction=0.5)\n",
    "data_test1_set_o1 = create_overlapping_sets(data_test1_o1, set_size=1000, overlap_fraction=0.5)\n",
    "data_test1_set_o2 = create_overlapping_sets(data_test1_o2, set_size=1000, overlap_fraction=0.5)\n",
    "data_test1_fft_oz = []\n",
    "data_test1_fft_o2 = []\n",
    "data_test1_fft_o1 = []\n",
    "for i in range(len(data_test1_set_oz)):\n",
    "    f, Pxx = welch(data_test1_set_oz[i], fs=250, nperseg= 250*4)\n",
    "    data_test1_fft_oz.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data_test1_set_o1[i], fs=250, nperseg= 250*4)\n",
    "    data_test1_fft_o1.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data_test1_set_o2[i], fs=250, nperseg= 250*4)\n",
    "    data_test1_fft_o2.append(Pxx[0:121])\n",
    "\n",
    "combined_test1 = np.hstack((data_test1_fft_oz, data_test1_fft_o1, data_test1_fft_o2))\n",
    "# labels_test1 = np.array([0]*len(data_test1_fft_oz))\n",
    "# print(combined_test1.shape)\n",
    "# print(labels_test1.shape)\n",
    "data_test2 = raw_test2[0:4,:]\n",
    "data_test2_oz = data_test2[0] - data_test2[1]\n",
    "data_test2_o1 = data_test2[2] - data_test2[1]\n",
    "data_test2_o2 = data_test2[3] - data_test2[1]\n",
    "data_test2_set_oz = create_overlapping_sets(data_test2_oz, set_size=1000, overlap_fraction=0.5)\n",
    "data_test2_set_o1 = create_overlapping_sets(data_test2_o1, set_size=1000, overlap_fraction=0.5)\n",
    "data_test2_set_o2 = create_overlapping_sets(data_test2_o2, set_size=1000, overlap_fraction=0.5)\n",
    "data_test2_fft_oz = []\n",
    "data_test2_fft_o2 = []\n",
    "data_test2_fft_o1 = []\n",
    "for i in range(len(data_test2_set_oz)):\n",
    "    f, Pxx = welch(data_test2_set_oz[i], fs=250, nperseg= 250*4)\n",
    "    data_test2_fft_oz.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data_test2_set_o1[i], fs=250, nperseg= 250*4)\n",
    "    data_test2_fft_o1.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data_test2_set_o2[i], fs=250, nperseg= 250*4)\n",
    "    data_test2_fft_o2.append(Pxx[0:121])\n",
    "\n",
    "combined_test2 = np.hstack((data_test2_fft_oz, data_test2_fft_o1, data_test2_fft_o2))\n",
    "# labels_test2 = np.array([0]*len(data_test2_fft_oz))\n",
    "# print(combined_test2.shape)\n",
    "# print(labels_test2.shape)\n",
    "\n",
    "data_test3 = raw_test3[0:4,:]\n",
    "data_test3_oz = data_test3[0] - data_test3[1]\n",
    "data_test3_o1 = data_test3[2] - data_test3[1]\n",
    "data_test3_o2 = data_test3[3] - data_test3[1]\n",
    "data_test3_set_oz = create_overlapping_sets(data_test3_oz, set_size=1000, overlap_fraction=0.5)\n",
    "data_test3_set_o1 = create_overlapping_sets(data_test3_o1, set_size=1000, overlap_fraction=0.5)\n",
    "data_test3_set_o2 = create_overlapping_sets(data_test3_o2, set_size=1000, overlap_fraction=0.5)\n",
    "data_test3_fft_oz = []\n",
    "data_test3_fft_o2 = []\n",
    "data_test3_fft_o1 = []\n",
    "for i in range(len(data_test3_set_oz)):\n",
    "    f, Pxx = welch(data_test3_set_oz[i], fs=250, nperseg= 250*4)\n",
    "    data_test3_fft_oz.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data_test3_set_o1[i], fs=250, nperseg= 250*4)\n",
    "    data_test3_fft_o1.append(Pxx[0:121])\n",
    "\n",
    "    f, Pxx = welch(data_test3_set_o2[i], fs=250, nperseg= 250*4)\n",
    "    data_test3_fft_o2.append(Pxx[0:121])\n",
    "\n",
    "combined_test3 = np.hstack((data_test3_fft_oz, data_test3_fft_o1, data_test3_fft_o2))\n",
    "# labels_test3 = np.array([0]*len(data_test3_fft_oz))\n",
    "# print(combined_test3.shape)\n",
    "# print(labels_test3.shape)\n",
    "\n",
    "# รวมข้อมูลจากทุก class เข้าด้วยกัน\n",
    "combined_test = np.vstack((combined_test1, combined_test2, combined_test3))\n",
    "\n",
    "  # ควรได้ (จำนวน samples ทั้งหมด, จำนวน features)\n",
    "\n",
    "# สร้าง label สำหรับแต่ละ class\n",
    "labels_test = np.array([0]*len(data_test1_fft_oz) + [1]*len(data_test2_fft_oz) + [2]*len(data_test3_fft_oz))\n",
    "# ตรวจสอบว่าข้อมูลมีขนาดที่ถูกต้อง\n",
    "print(combined_test.shape)\n",
    "print(labels_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_test = rf_classifier.predict(combined_test)\n",
    "y_pred_svm_test = svm_classifier.predict(combined_test)\n",
    "y_pred_lda_test = lda_classifier.predict(combined_test)\n",
    "y_pred_knn_test = knn_classifier.predict(combined_test)\n",
    "\n",
    "avg_accuracy_rf_test = accuracy_score(y_pred_rf_test, labels_test)\n",
    "avg_accuracy_svm_test = accuracy_score(y_pred_svm_test, labels_test)\n",
    "avg_accuracy_lda_test = accuracy_score(y_pred_lda_test, labels_test)\n",
    "avg_accuracy_knn_test = accuracy_score(y_pred_knn_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Precision_RF  Recall_RF     F1_RF  Precision_SVM  Recall_SVM    F1_SVM  \\\n",
      "Class                                                                           \n",
      "0          1.000000   0.965517  0.982456       0.805556        1.00  0.892308   \n",
      "1          0.961538   1.000000  0.980392       1.000000        0.72  0.837209   \n",
      "2          1.000000   1.000000  1.000000       1.000000        1.00  1.000000   \n",
      "\n",
      "       Precision_LDA  Recall_LDA  F1_LDA  Precision_KNN  Recall_KNN    F1_KNN  \n",
      "Class                                                                          \n",
      "0                1.0         1.0     1.0       0.828571        1.00  0.906250  \n",
      "1                1.0         1.0     1.0       1.000000        0.76  0.863636  \n",
      "2                1.0         1.0     1.0       1.000000        1.00  1.000000  \n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      " [[28  1  0]\n",
      " [ 0 25  0]\n",
      " [ 0  0 29]]\n",
      "\n",
      "Confusion Matrix for SVM:\n",
      " [[29  0  0]\n",
      " [ 7 18  0]\n",
      " [ 0  0 29]]\n",
      "\n",
      "Confusion Matrix for LDA:\n",
      " [[29  0  0]\n",
      " [ 0 25  0]\n",
      " [ 0  0 29]]\n",
      "\n",
      "Confusion Matrix for KNN:\n",
      " [[29  0  0]\n",
      " [ 6 19  0]\n",
      " [ 0  0 29]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming y_pred_rf_test, y_pred_svm_test, y_pred_lda_test, y_pred_knn_test, and labels_test are already defined\n",
    "\n",
    "def compute_metrics(y_true, y_pred, classes):\n",
    "    precision = precision_score(y_true, y_pred, average=None, labels=classes)\n",
    "    recall = recall_score(y_true, y_pred, average=None, labels=classes)\n",
    "    f1 = f1_score(y_true, y_pred, average=None, labels=classes)\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "    return precision, recall, f1, conf_matrix\n",
    "\n",
    "classes = np.unique(labels_test)\n",
    "\n",
    "# Calculate metrics for each classifier\n",
    "precision_rf, recall_rf, f1_rf, conf_matrix_rf = compute_metrics(labels_test, y_pred_rf_test, classes)\n",
    "precision_svm, recall_svm, f1_svm, conf_matrix_svm = compute_metrics(labels_test, y_pred_svm_test, classes)\n",
    "precision_lda, recall_lda, f1_lda, conf_matrix_lda = compute_metrics(labels_test, y_pred_lda_test, classes)\n",
    "precision_knn, recall_knn, f1_knn, conf_matrix_knn = compute_metrics(labels_test, y_pred_knn_test, classes)\n",
    "\n",
    "# Create DataFrames to display the results\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': classes,\n",
    "    'Precision_RF': precision_rf,\n",
    "    'Recall_RF': recall_rf,\n",
    "    'F1_RF': f1_rf,\n",
    "    'Precision_SVM': precision_svm,\n",
    "    'Recall_SVM': recall_svm,\n",
    "    'F1_SVM': f1_svm,\n",
    "    'Precision_LDA': precision_lda,\n",
    "    'Recall_LDA': recall_lda,\n",
    "    'F1_LDA': f1_lda,\n",
    "    'Precision_KNN': precision_knn,\n",
    "    'Recall_KNN': recall_knn,\n",
    "    'F1_KNN': f1_knn\n",
    "})\n",
    "\n",
    "metrics_df.set_index('Class', inplace=True)\n",
    "\n",
    "# Display the confusion matrices separately\n",
    "confusion_matrices = {\n",
    "    'Random Forest': conf_matrix_rf,\n",
    "    'SVM': conf_matrix_svm,\n",
    "    'LDA': conf_matrix_lda,\n",
    "    'KNN': conf_matrix_knn\n",
    "}\n",
    "\n",
    "print(metrics_df)\n",
    "\n",
    "# Print confusion matrices\n",
    "for clf_name, conf_matrix in confusion_matrices.items():\n",
    "    print(f\"\\nConfusion Matrix for {clf_name}:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Accuracy on test set: 0.42168674698795183\n"
     ]
    }
   ],
   "source": [
    "# Load โมเดลที่บันทึกไว้\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model(\"./final/model_FFT/cnn_model.h5\")\n",
    "\n",
    "# Load scaler ที่ใช้ในการปรับข้อมูลในขั้นตอนการฝึกโมเดล\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(combined_test)  # X_train เป็นข้อมูลที่ใช้ในการฝึกโมเดล\n",
    "\n",
    "# ปรับข้อมูลให้เป็นรูปแบบที่ใช้ได้กับโมเดล CNN\n",
    "X_test_scaled = scaler.transform(combined_test)\n",
    "X_test_reshaped = X_test_scaled.reshape(-1, 363, 1)\n",
    "\n",
    "# ทำนาย\n",
    "y_pred_test = np.argmax(loaded_model.predict(X_test_reshaped), axis=1)\n",
    "\n",
    "# ประเมินความแม่นยำ\n",
    "accuracy_test = accuracy_score(labels_test, y_pred_test)\n",
    "print(\"Accuracy on test set:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
